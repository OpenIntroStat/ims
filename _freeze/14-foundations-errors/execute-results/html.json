{
  "hash": "2e9137c86a86ca10a83884b8a1213fd3",
  "result": {
    "engine": "knitr",
    "markdown": "# Decision Errors {#sec-foundations-decision-errors}\n\n\n\n\n\n::: {.chapterintro data-latex=\"\"}\nUsing data to make inferential decisions about larger populations is not a perfect process.\nAs seen in [Chapter -@sec-foundations-randomization], a small p-value typically leads the researcher to a decision to reject the null claim or hypothesis.\nSometimes, however, data can produce a small p-value when the null hypothesis is actually true and the data are just inherently variable.\nHere we describe the errors which can arise in hypothesis testing, how to define and quantify the different errors, and suggestions for mitigating errors if possible.\n:::\n\n\\index{decision errors}\n\nHypothesis tests are not flawless.\nJust think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free.\nSimilarly, data can point to the wrong conclusion.\nHowever, what distinguishes statistical hypothesis tests from a court system is that our framework allows us to quantify and control how often the data lead us to the incorrect conclusion.\n\nIn a hypothesis test, there are two competing hypotheses: the null and the alternative.\nWe make a statement about which one might be true, but we might choose incorrectly.\nThere are four possible scenarios in a hypothesis test, which are summarized in @tbl-fourHTScenarios.\n\n\n::: {#tbl-fourHTScenarios .cell tbl-cap='Four different scenarios for hypothesis tests.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-condensed\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Test conclusion</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> Truth </th>\n   <th style=\"text-align:left;\"> Reject null hypothesis </th>\n   <th style=\"text-align:left;\"> Fail to reject null hypothesis </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;width: 15em; \"> Null hypothesis is true </td>\n   <td style=\"text-align:left;width: 8em; \"> Type 1 Error </td>\n   <td style=\"text-align:left;width: 8em; \"> Good decision </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 15em; \"> Alternative hypothesis is true </td>\n   <td style=\"text-align:left;width: 8em; \"> Good decision </td>\n   <td style=\"text-align:left;width: 8em; \"> Type 2 Error </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nA **Type 1 Error**\\index{Type 1 Error} is rejecting the null hypothesis when $H_0$ is actually true.\nSince we rejected the null hypothesis in the sex discrimination and opportunity cost studies, it is possible that we made a Type 1 Error in one or both of those studies.\nA **Type 2 Error**\\index{Type 2 Error} is failing to reject the null hypothesis when the alternative is actually true.\n\n\n\n\n\n::: {.workedexample data-latex=\"\"}\nIn a US court, the defendant is either innocent $(H_0)$ or guilty $(H_A).$ What does a Type 1 Error represent in this context?\nWhat does a Type 2 Error represent?\n@tbl-fourHTScenarios may be useful.\n\n------------------------------------------------------------------------\n\nIf the court makes a Type 1 Error, this means the defendant is innocent $(H_0$ true) but wrongly convicted.\nA Type 2 Error means the court failed to reject $H_0$ (i.e., failed to convict the person) when they were in fact guilty $(H_A$ true).\n:::\n\n::: {.guidedpractice data-latex=\"\"}\nConsider the opportunity cost study where we concluded students were less likely to make a DVD purchase if they were reminded that money not spent now could be spent later.\nWhat would a Type 1 Error represent in this context?[^14-foundations-errors-1]\n:::\n\n[^14-foundations-errors-1]: Making a Type 1 Error in this context would mean that reminding students that money not spent now can be spent later does not affect their buying habits, despite the strong evidence (the data suggesting otherwise) found in the experiment.\n    Notice that this does *not* necessarily mean something was wrong with the data or that we made a computational mistake.\n    Sometimes data simply point us to the wrong conclusion, which is why scientific studies are often repeated to check initial findings.\n\n::: {.workedexample data-latex=\"\"}\nHow could we reduce the Type 1 Error rate in US courts?\nWhat influence would this have on the Type 2 Error rate?\n\n------------------------------------------------------------------------\n\nTo lower the Type 1 Error rate, we might raise our standard for conviction from \"beyond a reasonable doubt\" to \"beyond a conceivable doubt\" so fewer people would be wrongly convicted.\nHowever, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type 2 Errors.\n:::\n\n::: {.guidedpractice data-latex=\"\"}\nHow could we reduce the Type 2 Error rate in US courts?\nWhat influence would this have on the Type 1 Error rate?[^14-foundations-errors-2]\n:::\n\n[^14-foundations-errors-2]: To lower the Type 2 Error rate, we want to convict more guilty people.\n    We could lower the standards for conviction from \"beyond a reasonable doubt\" to \"beyond a little doubt\".\n    Lowering the bar for guilt will also result in more wrongful convictions, raising the Type 1 Error rate.\n\n\\index{decision errors}\n\nThe example and guided practice above provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type.\n\n\\clearpage\n\n## Discernibility level\n\n\\index{discernibility level}\n\nThe **discernibility level** provides the cutoff for the p-value which will lead to a decision of \"reject the null hypothesis.\" Choosing a discernibility level for a test is important in many contexts, and the traditional level is 0.05.\nHowever, it is sometimes helpful to adjust the discernibility level based on the application.\nWe may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.\n\nIf making a Type 1 Error is dangerous or especially costly, we should choose a small discernibility level (e.g., 0.01 or 0.001).\nIf we want to be very cautious about rejecting the null hypothesis, we demand very strong evidence favoring the alternative $H_A$ before we would reject $H_0.$\n\nIf a Type 2 Error is relatively more dangerous or much more costly than a Type 1 Error, then we should choose a higher discernibility level (e.g., 0.10).\nHere we want to be cautious about failing to reject $H_0$ when the null is actually false.\n\n\n\n\n\n::: {.tip data-latex=\"\"}\n**Discernibility levels should reflect consequences of errors.**\n\nThe discernibility level selected for a test should reflect the real-world consequences associated with making a Type 1 or Type 2 Error.\n:::\n\n## Two-sided hypotheses {#sec-two-sided-hypotheses}\n\n\\index{hypothesis testing}\n\nIn [Chapter -@sec-foundations-randomization] we explored whether women were discriminated against and whether a simple trick could make students a little thriftier.\nIn these two case studies, we have actually ignored some possibilities:\n\n-   What if *men* are actually discriminated against?\n-   What if the money trick actually makes students *spend more*?\n\nThese possibilities weren't considered in our original hypotheses or analyses.\nThe disregard of the extra alternatives may have seemed natural since the data pointed in the directions in which we framed the problems.\nHowever, there are two dangers if we ignore possibilities that disagree with our data or that conflict with our world view:\n\n1.  Framing an alternative hypothesis simply to match the direction that the data point will generally inflate the Type 1 Error rate.\n    After all the work we have done (and will continue to do) to rigorously control the error rates in hypothesis tests, careless construction of the alternative hypotheses can disrupt that hard work.\n\n2.  If we only use alternative hypotheses that agree with our worldview, then we are going to be subjecting ourselves to **confirmation bias**\\index{confirmation bias}, which means we are looking for data that supports our ideas.\n    That's not very scientific, and we can do better!\n\nThe original hypotheses we have seen are called **one-sided hypothesis tests**\\index{one-sided hypothesis test} because they only explored one direction of possibilities.\nSuch hypotheses are appropriate when we are exclusively interested in the single direction, but usually we want to consider all possibilities.\nTo do so, let's learn about **two-sided hypothesis tests**\\index{two-sided hypothesis test} in the context of a new study that examines the impact of using blood thinners on patients who have undergone CPR.\n\n\n\n\n\nCardiopulmonary resuscitation (CPR) is a procedure used on individuals suffering a heart attack when other emergency resources are unavailable.\nThis procedure is helpful in providing some blood circulation to keep a person alive, but CPR chest compression can also cause internal injuries.\nInternal bleeding and other injuries that can result from CPR complicate additional treatment efforts.\nFor instance, blood thinners may be used to help release a clot that is causing the heart attack once a patient arrives in the hospital.\nHowever, blood thinners negatively affect internal injuries.\n\nHere we consider an experiment with patients who underwent CPR for a heart attack and were subsequently admitted to a hospital.\nEach patient was randomly assigned to either receive a blood thinner (treatment group) or not receive a blood thinner (control group).\nThe outcome variable of interest was whether the patient survived for at least 24 hours.\n[@Bottiger:2001]\n\n::: {.data data-latex=\"\"}\nThe [`cpr`](http://openintrostat.github.io/openintro/reference/cpr.html) data can be found in the [**openintro**](http://openintrostat.github.io/openintro) R package.\n:::\n\n::: {.workedexample data-latex=\"\"}\nForm hypotheses for this study in plain and statistical language.\nLet $p_C$ represent the true survival rate of people who do not receive a blood thinner (corresponding to the control group) and $p_T$ represent the survival rate for people receiving a blood thinner (corresponding to the treatment group).\n\n------------------------------------------------------------------------\n\nWe want to understand whether blood thinners are helpful or harmful.\nWe'll consider both of these possibilities using a two-sided hypothesis test.\n\n-   $H_0:$ Blood thinners do not have an overall survival effect, i.e., the survival proportions are the same in each group.\n    $p_T - p_C = 0.$\n\n-   $H_A:$ Blood thinners have an impact on survival, either positive or negative, but not zero.\n    $p_T - p_C \\neq 0.$\n\nNote that if we had done a one-sided hypothesis test, the resulting hypotheses would have been:\n\n-   $H_0:$ Blood thinners do not have a positive overall survival effect, i.e., the survival proportions for the blood thinner group is the same or lower than the control group.\n    $p_T - p_C \\leq 0.$\n\n-   $H_A:$ Blood thinners have a positive impact on survival.\n    $p_T - p_C > 0.$\n:::\n\nThere were 50 patients in the experiment who did not receive a blood thinner and 40 patients who did.\nThe study results are shown in @tbl-cpr-summary.\n\n\n::: {#tbl-cpr-summary .cell tbl-cap='Results for the CPR study. Patients in the treatment group were given a\nblood thinner, and patients in the control group were not.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-condensed\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Group </th>\n   <th style=\"text-align:center;\"> Died </th>\n   <th style=\"text-align:center;\"> Survived </th>\n   <th style=\"text-align:center;\"> Total </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;width: 7em; \"> Control </td>\n   <td style=\"text-align:center;width: 7em; \"> 39 </td>\n   <td style=\"text-align:center;width: 7em; \"> 11 </td>\n   <td style=\"text-align:center;width: 7em; \"> 50 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 7em; \"> Treatment </td>\n   <td style=\"text-align:center;width: 7em; \"> 26 </td>\n   <td style=\"text-align:center;width: 7em; \"> 14 </td>\n   <td style=\"text-align:center;width: 7em; \"> 40 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;width: 7em; \"> Total </td>\n   <td style=\"text-align:center;width: 7em; \"> 65 </td>\n   <td style=\"text-align:center;width: 7em; \"> 25 </td>\n   <td style=\"text-align:center;width: 7em; \"> 90 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n::: {.guidedpractice data-latex=\"\"}\nWhat is the observed survival rate in the control group?\nAnd in the treatment group?\nAlso, provide a point estimate $(\\hat{p}_T - \\hat{p}_C)$ for the true difference in population survival proportions across the two groups: $p_T - p_C.$[^14-foundations-errors-3]\n:::\n\n[^14-foundations-errors-3]: Observed control survival rate: $\\hat{p}_C = \\frac{11}{50} = 0.22.$ Treatment survival rate: $\\hat{p}_T = \\frac{14}{40} = 0.35.$ Observed difference: $\\hat{p}_T - \\hat{p}_C = 0.35 - 0.22 = 0.13.$\n\nAccording to the point estimate, for patients who have undergone CPR outside of the hospital, an additional 13% of these patients survive when they are treated with blood thinners.\nHowever, we wonder if this difference could be easily explainable by chance, if the treatment has no effect on survival.\n\nAs we did in past studies, we will simulate what type of differences we might see from chance alone under the null hypothesis.\nBy randomly assigning each of the patient's files to a \"simulated treatment\" or \"simulated control\" allocation, we get a new grouping.\nIf we repeat this simulation 1,000 times, we can build a **null distribution**\\index{null distribution} of the differences shown in @fig-CPR-study-right-tail.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Null distribution of the point estimate for the difference in proportions,\n$\\hat{p}_T - \\hat{p}_C.$ The shaded right tail shows observations that are\nat least as large as the observed difference, 0.13.\n](14-foundations-errors_files/figure-html/fig-CPR-study-right-tail-1.png){#fig-CPR-study-right-tail fig-alt='Histogram of the null distribution of the point estimate for the difference\nin proportions, $\\hat{p}_T - \\hat{p}_C.$ The shaded right tail shows\nobservations that are at least as large as the observed difference, 0.13.' width=90%}\n:::\n:::\n\n\nThe right tail area is 0.135.\n(Note: it is only a coincidence that we also have $\\hat{p}_T - \\hat{p}_C=0.13.)$ However, contrary to how we calculated the p-value in previous studies, the p-value of this test is not actually the tail area we calculated, i.e., it's not 0.135!\n\nThe p-value is defined as the probability we observe a result at least as favorable to the alternative hypothesis as the result (i.e., the difference) we observe.\nIn this case, any differences less than or equal to -0.13 would also provide equally strong evidence favoring the alternative hypothesis as a difference of +0.13 did.\nA difference of -0.13 would correspond to 13% higher survival rate in the control group than the treatment group.\nIn @fig-CPR-study-p-value we have also shaded these differences in the left tail of the distribution.\nThese two shaded tails provide a visual representation of the p-value for a two-sided test.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Null distribution of the point estimate for the difference in proportions,\n$\\hat{p}_T - \\hat{p}_C.$ All values that are at least as extreme as +0.13\nbut in either direction away from 0 are shaded.\n](14-foundations-errors_files/figure-html/fig-CPR-study-p-value-1.png){#fig-CPR-study-p-value fig-alt='Histogram of the null distribution of the point estimate for the difference\nin proportions, $\\hat{p}_T - \\hat{p}_C.$ All values that are at least as\nextreme as +0.13 but in either direction away from 0 are shaded.' width=90%}\n:::\n:::\n\n\nFor a two-sided test, take the single tail (in this case, 0.131) and double it to get the p-value: 0.262.\nSince this p-value is larger than 0.05, we do not reject the null hypothesis.\nThat is, we do not find convincing evidence that the blood thinner has any influence on survival of patients who undergo CPR prior to arriving at the hospital.\n\n::: {.important data-latex=\"\"}\n**Default to a two-sided test.**\n\nWe want to be rigorous and keep an open mind when we analyze data and evidence.\nUse a one-sided hypothesis test only if you truly have interest in only one direction.\n:::\n\n::: {.important data-latex=\"\"}\n**Computing a p-value for a two-sided test.**\n\nFirst compute the p-value for one tail of the distribution, then double that value to get the two-sided p-value.\nThat's it!\n:::\n\n::: {.workedexample data-latex=\"\"}\nConsider the situation of the medical consultant.\nNow that you know about one-sided and two-sided tests, which type of test do you think is more appropriate?\n\n------------------------------------------------------------------------\n\nThe setting has been framed in the context of the consultant being helpful (which is what led us to a one-sided test originally), but what if the consultant actually performed *worse* than the average?\nWould we care?\nMore than ever!\nSince it turns out that we care about a finding in either direction, we should run a two-sided test.\nThe p-value for the two-sided test is double that of the one-sided test, here the simulated p-value would be 0.2444.\n:::\n\nGenerally, to find a two-sided p-value we double the single tail area, which remains a reasonable approach even when the distribution is asymmetric.\nHowever, the approach can result in p-values larger than 1 when the point estimate is very near the mean in the null distribution; in such cases, we write that the p-value is 1.\nAlso, very large p-values computed in this way (e.g., 0.85), may also be slightly inflated.\nTypically, we do not worry too much about the precision of very large p-values because they lead to the same analysis conclusion, even if the value is slightly off.\n\n\\clearpage\n\n## Controlling the Type 1 Error rate\n\nNow that we understand the difference between one-sided and two-sided tests, we must recognize when to use each type of test.\nBecause of the result of increased error rates, it is never okay to change two-sided tests to one-sided tests after observing the data.\nWe explore the consequences of ignoring this advice in the next example.\n\n::: {.workedexample data-latex=\"\"}\nUsing $\\alpha=0.05,$ we show that freely switching from two-sided tests to one-sided tests will lead us to make twice as many Type 1 Errors as intended.\n\n------------------------------------------------------------------------\n\nSuppose we are interested in finding any difference from 0.\nWe've created a smooth-looking **null distribution** representing differences due to chance in @fig-type1ErrorDoublingExampleFigure.\n\nSuppose the sample difference was larger than 0.\nThen if we can flip to a one-sided test, we would use $H_A:$ difference $> 0.$ Now if we obtain any observation in the upper 5% of the distribution, we would reject $H_0$ since the p-value would just be a the single tail.\nThus, if the null hypothesis is true, we incorrectly reject the null hypothesis about 5% of the time when the sample mean is above the null value, as shown in @fig-type1ErrorDoublingExampleFigure.\n\nSuppose the sample difference was smaller than 0.\nThen if we change to a one-sided test, we would use $H_A:$ difference $< 0.$ If the observed difference falls in the lower 5% of the figure, we would reject $H_0.$ That is, if the null hypothesis is true, then we would observe this situation about 5% of the time.\n\nBy examining these two scenarios, we can determine that we will make a Type 1 Error $5\\%+5\\%=10\\%$ of the time if we are allowed to swap to the \"best\" one-sided test for the data.\nThis is twice the error rate we prescribed with our discernibility level: $\\alpha=0.05$ (!).\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![The shaded regions represent areas where we would reject $H_0$ under the\nbad practices considered in when $\\alpha = 0.05.$\n](14-foundations-errors_files/figure-html/fig-type1ErrorDoublingExampleFigure-1.png){#fig-type1ErrorDoublingExampleFigure fig-alt='Density curve of a normal distribution with mean 0 and standard deviation 1.\nThe shaded regions represent areas where we would reject $H_0$ under the\nbad practices considered in when $\\alpha = 0.05.$\n' width=90%}\n:::\n:::\n\n\n::: caution\n**Hypothesis tests should be set up *before* seeing the data.**\n\nAfter observing data, it is tempting to turn a two-sided test into a one-sided test.\nAvoid this temptation.\nHypotheses should be set up *before* observing the data.\n:::\n\n\\index{hypothesis testing}\n\n\\clearpage\n\n## Power {#sec-pow}\n\nAlthough we won't go into extensive detail here, power is an important topic for follow-up consideration after understanding the basics of hypothesis testing.\nA good power analysis is a vital preliminary step to any study as it will inform whether the data you collect are sufficient for being able to conclude your research broadly.\n\nOften times in experiment planning, there are two competing considerations:\n\n-   We want to collect enough data that we can detect important effects.\n-   Collecting data can be expensive, and, in experiments involving people, there may be some risk to patients.\n\nWhen planning a study, we want to know how likely we are to detect an effect we care about.\nIn other words, if there is a real effect, and that effect is large enough that it has practical value, then what is the probability that we detect that effect?\nThis probability is called the **power**\\index{power}, and we can compute it for different sample sizes or different effect sizes.\n\n::: {.important data-latex=\"\"}\n**Power.**\n\nThe power of the test is the probability of rejecting the null claim when the alternative claim is true.\n\nHow easy it is to detect the effect depends on both how big the effect is (e.g., how good the medical treatment is) as well as the sample size.\n:::\n\nWe think of power as the probability that you will become rich and famous from your science.\nIn order for your science to make a splash, you need to have good ideas!\nThat is, you won't become famous if you happen to find a single Type 1 error which rejects the null hypothesis.\nInstead, you'll become famous if your science is very good and important (that is, if the alternative hypothesis is true).\nThe better your science is (i.e., the better the medical treatment), the larger the *effect size* and the easier it will be for you to convince people of your work.\n\nNot only does your science need to be solid, but you also need to have evidence (i.e., data) that shows the effect.\nA few observations (e.g., $n = 2)$ is unlikely to be convincing because of well known ideas of natural variability.\nIndeed, the larger the dataset which provides evidence for your scientific claim, the more likely you are to convince the community that your idea is correct.\n\n\n\n\n\n\\clearpage\n\n## Chapter review {#sec-chp15-review}\n\n### Summary\n\nAlthough hypothesis testing provides a strong framework for making decisions based on data, as the analyst, you need to understand how and when the process can go wrong.\nThat is, always keep in mind that the conclusion to a hypothesis test may not be right!\nSometimes when the null hypothesis is true, we will accidentally reject it and commit a type 1 error; sometimes when the alternative hypothesis is true, we will fail to reject the null hypothesis and commit a type 2 error.\nThe power of the test quantifies how likely it is to obtain data which will reject the null hypothesis when indeed the alternative is true; the power of the test is increased when larger sample sizes are taken.\n\n### Terms\n\nWe introduced the following terms in the chapter.\nIf you're not sure what some of these terms mean, we recommend you go back in the text and review their definitions.\nWe are purposefully presenting them in alphabetical order, instead of in order of appearance, so they will be a little more challenging to locate.\nHowever, you should be able to easily spot them as **bolded text**.\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> confirmation bias </td>\n   <td style=\"text-align:left;\"> one-sided hypothesis test </td>\n   <td style=\"text-align:left;\"> two-sided hypothesis test </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> discernibility level </td>\n   <td style=\"text-align:left;\"> power </td>\n   <td style=\"text-align:left;\"> type 1 error </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> null distribution </td>\n   <td style=\"text-align:left;\"> significance level </td>\n   <td style=\"text-align:left;\"> type 2 error </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\\clearpage\n\n## Exercises {#sec-chp14-exercises}\n\nAnswers to odd-numbered exercises can be found in [Appendix -@sec-exercise-solutions-14].\n\n::: {.exercises data-latex=\"\"}\n1.  **Testing for Fibromyalgia.** A patient named Diana was diagnosed with Fibromyalgia, a long-term syndrome of body pain, and was prescribed anti-depressants.\n    Being the skeptic that she is, Diana didn't initially believe that anti-depressants would help her symptoms.\n    However, after a couple months of being on the medication she decides that the anti-depressants are working, because she feels like her symptoms are in fact getting better.\n\n    a.  Write the hypotheses in words for Diana's skeptical position when she started taking the anti-depressants.\n\n    b.  What is a Type 1 Error in this context?\n\n    c.  What is a Type 2 Error in this context?\n\n2.  **Which is higher?** In each part below, there is a value of interest and two scenarios: (i) and (ii).\n    For each part, report if the value of interest is larger under scenario (i), scenario (ii), or whether the value is equal under the scenarios.\n\n    a.  The standard error of $\\hat{p}$ when (i) $n = 125$ or (ii) $n = 500$.\n\n    b.  The margin of error of a confidence interval when the confidence level is (i) 90% or (ii) 80%.\n\n    c.  The p-value for a Z-statistic of 2.5 calculated based on a (i) sample with $n = 500$ or based on a (ii) sample with $n = 1000$.\n\n    d.  The probability of making a Type 2 Error when the alternative hypothesis is true and the discernibility level is (i) 0.05 or (ii) 0.10.\n\n3.  **Testing for food safety.** A food safety inspector is called upon to investigate a restaurant with a few customer reports of poor sanitation practices.\n    The food safety inspector uses a hypothesis testing framework to evaluate whether regulations are not being met.\n    If he decides the restaurant is in gross violation, its license to serve food will be revoked.\n\n    a.  Write the hypotheses in words.\n\n    b.  What is a Type 1 Error in this context?\n\n    c.  What is a Type 2 Error in this context?\n\n    d.  Which error is more problematic for the restaurant owner?\n        Why?\n\n    e.  Which error is more problematic for the diners?\n        Why?\n\n    f.  As a diner, would you prefer that the food safety inspector requires strong evidence or very strong evidence of health concerns before revoking a restaurant's license?\n        Explain your reasoning.\n\n4.  **True or false.** Determine if the following statements are true or false, and explain your reasoning.\n    If false, state how it could be corrected.\n\n    a.  If a given value (for example, the null hypothesized value of a parameter) is within a 95% confidence interval, it will also be within a 99% confidence interval.\n\n    b.  Decreasing the discernibility level ($\\alpha$) will increase the probability of making a Type 1 Error.\n\n    c.  Suppose the null hypothesis is $p = 0.5$ and we fail to reject $H_0$.\n        Under this scenario, the true population proportion is 0.5.\n\n    d.  With large sample sizes, even small differences between the null value and the observed point estimate, a difference often called the effect size, will be identified as statistically discernible.\n\n    \\clearpage\n\n5.  **Online communication.** A study suggests that 60% of college student spend 10 or more hours per week communicating with others online.\n    You believe that this is incorrect and decide to collect your own sample for a hypothesis test.\n    You randomly sample 160 students from your dorm and find that 70% spent 10 or more hours a week communicating with others online.\n    A friend of yours, who offers to help you with the hypothesis test, comes up with the following set of hypotheses.\n    Indicate any errors you see.\n\n    $$H_0: \\hat{p} < 0.6 \\quad \\quad H_A: \\hat{p} > 0.7$$\n\n6.  **Same observation, different sample size.** Suppose you conduct a hypothesis test based on a sample where the sample size is $n = 50$, and arrive at a p-value of 0.08.\n    You then refer back to your notes and discover that you made a careless mistake, the sample size should have been $n = 500$.\n    Will your p-value increase, decrease, or stay the same?\n    Explain.\n\n\n:::\n",
    "supporting": [
      "14-foundations-errors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n<link href=\"site_libs/bsTable-3.3.7/bootstrapTable.min.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/bsTable-3.3.7/bootstrapTable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}