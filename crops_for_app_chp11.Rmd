## Randomization test for $H_0: \beta_1= 0$ {#randslope}

Consider the data on Global Crop Yields compiled by [Our World in Data](https://ourworldindata.org/crop-yields) and presented as part of the [TidyTuesday](https://github.com/rfordatascience/tidytuesday/trunk/data/2020/2020-09-01) series seen in Figure \@ref(fig:allcrops).
The scientific research interest at hand will be in determining the linear relationship between wheat yield (for a country-year) and other crop yields.
The dataset is quite rich and deserves exploring, but for this example, we will focus only on the annual crop yield in the United States.

```{r include=FALSE}
terms_chp_25 <- c(terms_chp_25, "randomization test for the slope")
```

```{r allcrops, echo = FALSE, fig.cap = "Yield (in tonnes per hectare) for six different crops in the US.  The color of the dot indicates the year."}
key_crop_yields <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv") %>%
  rename(
    wheat = `Wheat (tonnes per hectare)`,
    rice = `Rice (tonnes per hectare)`,
    maize = `Maize (tonnes per hectare)`,
    soybeans = `Soybeans (tonnes per hectare)`,
    potatoes = `Potatoes (tonnes per hectare)`,
    beans = `Beans (tonnes per hectare)`,
    peas = `Peas (tonnes per hectare)`,
    cassava = `Cassava (tonnes per hectare)`,
    barley = `Barley (tonnes per hectare)`,
    cocoa = `Cocoa beans (tonnes per hectare)`,
    bananas = `Bananas (tonnes per hectare)`
  )

crops_USA <- key_crop_yields %>%
  filter(Code == "USA") %>%
  select(-cassava, -cocoa, -potatoes, -bananas)

crops_USA_long <- crops_USA %>%
  pivot_longer(rice:barley,
    names_to = "crop", values_to = "yield"
  )

ggplot(crops_USA_long) +
  geom_point(aes(x = yield, y = wheat, color = Year)) +
  facet_wrap(~crop) +
  ylim(c(1.4, 3.6)) +
  scale_color_gradient(low = IMSCOL["blue", "f3"], high = IMSCOL["blue", "full"])
```

As you have seen previously, statistical inference typically relies on setting a null hypothesis which is hoped to be subsequently rejected.
In the linear model setting, we might hope to have a linear relationship between `maize` and `wheat` in settings where `maize` production is known and `wheat` production needs to be predicted.

The relevant hypotheses for the linear model setting can be written in terms of the population slope parameter.
Here the population refers to a larger set of years where `maize` and `wheat` are both grown in the US.

-   $H_0: \beta_1= 0$, there is no linear relationship between `wheat` and `maize`.
-   $H_A: \beta_1 \ne 0$, there is some linear relationship between `wheat` and `maize`.

Recall that for the randomization test, we permute one variables to eliminate any existing relationship between the variables.
That is, we set the null hypothesis to be true, and we measure the natural variability in the data due to sampling but **not** due to variables being correlated.
Figure \@ref(fig:permwheatScatter) shows the observed data and a scatterplot of one permutation of the `wheat` variable.
The careful observer can see that each of the observed the values for `wheat` (and for `maize`) exist in both the original data plot as well as the permuted `wheat` plot, but the given `wheat` and `maize` yields are no longer matched for a given year.
That is, each `wheat` yield is randomly assigned to a new `maize` yield.

```{r permwheatScatter, fig.cap = "Original (left) and permuted (right) data.  The permutation removes the linear relationship between `wheat` and `maize`.  Repeated permutations allow for quantifying the variability in the slope under the condition that there is no linear relationship (i.e., that the null hypothesis is true).", fig.show = 'hold', out.width='47%', fig.ncol = 2}
set.seed(4747)
ggplot(crops_USA) +
  geom_point(aes(x = maize, y = wheat)) +
  ggtitle("Original Data") +
  ylim(c(1.4, 3.6))

ggplot(crops_USA) +
  geom_point(aes(x = maize, y = sample(wheat))) +
  ylab("permuted wheat") +
  ggtitle("Permuted Data") +
  ylim(c(1.4, 3.6))
```

By repeatedly permuting the response variable any pattern in the linear model that is observed is due only to random chance (and not an underlying relationship).
The randomization test compares the slopes calculated from the permuted response variable with the observed slope.
If the observed slope is inconsistent with the slopes from permuting, we can conclude that there is some underlying relationship (and that the slope is not merely due to random chance).

### Observed data

We will continue to use the crop data to investigate the linear relationship between `wheat` and `maize`.
Note that the least squares model (see Chapter \@ref(model-slr)) describing the relationship is given in Table \@ref(tab:lsCrops).
The columns in Table \@ref(tab:lsCrops) are further described in Section \@ref(mathslope).

```{r lsCrops}
lm(wheat ~ maize, data = crops_USA) %>%
  tidy() %>%
  kable(caption = "The least squares estimates of the intercept and slope are given in the `estimate` column.  The observed slope is 0.195.") %>%
  kable_styling()
```

### Variability of the statistic

After permuting the data, the least squares estimate of the line can be computed.
Repeated permutations and slope calculations describe the variability in the line (i.e., in the slope) due only to the natural variability and not due to a relationship between `wheat` and `maize`.
Figure \@ref(fig:permwheatlm) shows two different permutations of `wheat` and the resulting linear models.

```{r permwheatlm, fig.cap = "Two different permutations of the wheat variable with slightly different least squares regression lines.", fig.show = 'hold', fig.ncol = 2, out.width='47%'}
set.seed(47)
ggplot(crops_USA, aes(x = maize, y = sample(wheat))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  ylab("permuted wheat") +
  ggtitle("First Permutation of Wheat") +
  ylim(c(1.4, 3.6))

ggplot(crops_USA, aes(x = maize, y = sample(wheat))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, fullrange = TRUE) +
  ylab("permuted wheat") +
  ggtitle("Second Permutation of Wheat") +
  ylim(c(1.4, 3.6))
```

As you can see, sometimes the slope of the permuted data is positive, sometimes it is negative.
Because the randomization happens under the condition of no underlying relationship (because the response variable is completely mixed with the explanatory variable), we expect to see the center of the randomized slope distribution to be zero.

### Observed statistic vs. null statistics

```{r nulldistCrop, echo=FALSE, fig.cap = "Histogram of slopes given different permutations of the wheat variable.  The vertical red line is at the observed value of the slope, 0.195."}
perm_slope <- crops_USA %>%
  specify(wheat ~ maize) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "slope")

obs_slope <- crops_USA %>%
  specify(wheat ~ maize) %>%
  calculate(stat = "slope") %>%
  pull()

ggplot(data = perm_slope, aes(x = stat)) +
  geom_histogram() +
  geom_vline(xintercept = obs_slope, color = IMSCOL["red", "full"])
```

As we can see from Figure \@ref(fig:nulldistCrop), a slope estimate as extreme as the observed slope estimate (the red line) never happened in many repeated permutations of the `wheat` variable.
That is, if indeed there were no linear relationship between `wheat` and `maize`, the natural variability of the slopes would produce estimates between approximately -0.1 and +0.1.
We reject the null hypothesis.
Therefore, we believe that the slope observed on the original data is not just due to natural variability and indeed, there is a linear relationship between `wheat` and `maize` crop yield in the US.

## Bootstrap confidence interval for $\beta_1$ {#bootbeta1}

As we have seen in previous chapters, we can use bootstrapping to estimate the sampling distribution of the statistic of interest (here, the slope) without the null assumption of no relationship (which was the condition in the randomization test).
Because interest is now in creating a CI, there is no null hypothesis, so there won't be any reason to permute either of the variables.

```{r include=FALSE}
terms_chp_25 <- c(terms_chp_25, "bootstrap CI for the slope")
```

### Observed data

Returning to the crop data, we may want to consider the relationship between `peas` and `wheat`.
Are `peas` a good predictor of `wheat`?
And if so, what is their relationship?
That is, what is the slope that models `average wheat yield` as a function of `peas`?

```{r echo = FALSE}
set.seed(4747)
crops4 <- crops_USA %>%
  sample_n(size = 58, replace = TRUE)
crops5 <- crops_USA %>%
  sample_n(size = 58, replace = TRUE)
crops_many_BS <- crops_USA %>%
  rep_sample_n(size = 58, replace = TRUE, reps = 50)
```

```{r peasPlot, echo = FALSE, fig.cap = "Original data: wheat yield as a linear model of peas yield, in tonnes per hectare.  Notice that the relationship between `peas` and `wheat` is not as strong as the relationship we saw previously between `maize` and `wheat`."}
ggplot(crops_USA) +
  geom_point(aes(x = peas, y = wheat)) +
  geom_smooth(aes(x = peas, y = wheat),
    method = "lm", se = FALSE,
    fullrange = TRUE, 
  ) +
  coord_cartesian(ylim = c(1.4, 3.6))
```

### Variability of the statistic

Because we are not focused on a null distribution, we sample with replacement $n=58$ observations from the original dataset.
Recall that with bootstrapping we always resample the same number of observations as we start with in order to mimic the process of taking a sample from the population.
When sampling in the linear model case, consider each observation to be a single dot.
If the dot is resampled, both the `wheat` and the `peas` measurement are observed.
The measurements are linked to the dot (i.e., to the year in which the measurements were taken).

```{r crop2BS, echo = FALSE, fig.cap = "Original and one bootstrap sample of the crop data.  Note that it is difficult to differentiate the two plots, as (within a single bootstrap sample) the observations which have been resampled twice are plotted as points on top of one another.  The orange circle represent points in the original data which were not included in the bootstrap sample.  The blue circle represents a point that was repeatedly resampled (and is therefore darker) in the bootstrap sample.  The green circle represents a particular structure to the data which is observed in both the original and bootstrap samples.",  fig.show = 'hold', out.width = '47%', fig.ncol = 2}

ggplot(crops_USA) +
  geom_point(aes(x = maize, y = wheat), alpha = 0.4) +
  geom_smooth(aes(x = maize, y = wheat),
    method = "lm", se = FALSE,
    fullrange = TRUE
  ) +
  ggtitle("Original crop data.") +
  geom_point(x = 8.38, y = 2.89, color = IMSCOL["green", "full"], pch = 1, size = 8) +
  geom_point(x = 4.15, y = 1.7, color = IMSCOL["red", "full"], pch = 1, size = 8) +
  geom_point(x = 5.53, y = 2.28, pch = 1, size = 8) +
  ylim(c(1.4, 3.6))


ggplot(crops5) +
  geom_point(aes(x = maize, y = wheat), alpha = 0.4) +
  geom_smooth(aes(x = maize, y = wheat),
    method = "lm", se = FALSE,
    fullrange = TRUE
  ) +
  ggtitle("Bootstrap sample from the crop data.") +
  geom_point(x = 8.38, y = 2.89, color = IMSCOL["green", "full"], pch = 1, size = 8) +
  geom_point(x = 4.15, y = 1.7, color = IMSCOL["red", "full"], pch = 1, size = 8) +
  geom_point(x = 5.53, y = 2.28, pch = 1, size = 8) +
  ylim(c(1.4, 3.6))
```

Figure \@ref(fig:crop2BS) shows the original data as compared with a single bootstrap sample, resulting in (slightly) different linear models.
The orange circle represent points in the original data which were not included in the bootstrap sample.
The blue circle represents a point that was repeatedly resampled (and is therefore darker) in the bootstrap sample.
The green circle represents a particular structure to the data which is observed in both the original and bootstrap samples.
By repeatedly resampling, we can see dozens of bootstrapped slopes on the same plot in Figure \@ref(fig:cropBS).

```{r cropBS, echo = FALSE, fig.cap = "Repeated bootstrap resamples of size 58 are taken from the original data.  Each of the bootstrapped linear model is slightly different."}
ggplot(crops_many_BS, aes(x = peas, y = wheat, group = replicate)) +
  # geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = IMSCOL[1, 1], fullrange = TRUE) +
  ylim(c(1.4, 3.6))
```

Recall that in order to create a confidence interval for the slope, we need to find the range of values that the statistic (here the slope) takes on from different bootstrap samples.
Figure \@ref(fig:peasBSslopes) is a histogram of the relevant bootstrapped slopes.
We can see that a 95% bootstrap percentile interval for the true population slope is given by (0.061, 0.52).
We are 95% confident that for the model describing the population of crops of `peas` and `wheat`, a one unit increase in `peas` yield (in tonnes per hectare) will be associated with an increase in predicted average `wheat` yield of between 0.061 and 0.52 tonnes per hectare.

```{r peasBSslopes, fig.cap = "The original crop data on wheat and peas is bootstrapped 1,000 times. The histogram provides a sense for the variability of the slope of the linear model slope from sample to sample.", warning=FALSE, fig.width=15}

set.seed(4747)
crops_many_lm_BS <- crops_many_BS %>%
  group_by(replicate) %>%
  do(tidy(lm(wheat ~ peas, data = .))) %>%
  filter(term == "peas")

lower <- round(quantile(crops_many_lm_BS$estimate, 0.025), 3)
upper <- round(quantile(crops_many_lm_BS$estimate, 0.975), 2)

ggplot(crops_many_lm_BS, aes(x = estimate)) +
  geom_histogram(binwidth = 0.04) +
  annotate("segment", x = lower, xend = lower, y = 0, yend = 6, linetype = "dashed") +
  annotate("segment", x = upper, xend = upper, y = 0, yend = 6, linetype = "dashed") +
  annotate("text", x = lower, y = 7, label = "2.5 percentile\n0.061", size = 6) +
  annotate("text", x = upper, y = 7, label = "97.5 percentile\n0.52", size = 6) +
  labs(
    x = "Bootstrapped values of the slope statistic describing wheat and peas",
    y = NULL
  ) +
  theme(axis.text.y = element_blank())
```
