1.  **GPA, mathematical interval.** A survey of 55 Duke University students asked about their GPA (`gpa`), number of hours they study weekly (`studyweek`), number of hours they sleep nightly (`sleepnight`), and whether they go out more than two nights a week (`out_mt2`).
    We use these data to build a model predicting GPA from the other variables.
    Summary of the model is shown below.
    Note that `out_mt2` is `1` if the student goes out more than two nights a week, and 0 otherwise.[^_25-ex-inf-model-mlr-1]

    ```{r}
    openintro::gpa |>
      mutate(out_mt2 = if_else(out > 2, 1, 0)) |>
      lm(gpa ~ studyweek + sleepnight + out_mt2, data = _) |>
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 3) |>
      kable_styling(bootstrap_options = c("striped", "condensed"),
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```

    a.  Calculate a 95% confidence interval for the coefficient of `out_mt2` (go out more than two night a week) in the model, and interpret it in the context of the data.

    b.  Would you expect a 95% confidence interval for the slope of the remaining variables to include 0? Explain your reasoning.

\vfill

2.  **Tourism spending.** The Association of Turkish Travel Agencies reports the number of foreign tourists visiting Turkey and tourist spending by year.
    Three plots are provided: scatterplot showing the relationship between these two variables along with the least squares fit, residuals plot, and histogram of residuals.[^_25-ex-inf-model-mlr-3]

    ```{r}
    #| out-width: 100%
    #| fig-asp: 0.4
    p_1 <- ggplot(tourism, aes(x = visitor_count_tho, y = tourist_spending)) +
      geom_smooth(method = "lm", color = "darkgray", se = FALSE) +
      geom_point(size = 3, alpha = 0.6) +
      labs(
        x = "Number of tourists \n(thousands)",
        y = "Spending (million $)"
      )

    m <- lm(tourist_spending ~ visitor_count_tho, data = tourism)
    m_aug <- augment(m)

    p_2 <- ggplot(m_aug, aes(x = .fitted, y = .resid)) +
      geom_hline(yintercept = 0, linetype = "dashed") +
      geom_point(size = 3, alpha = 0.6) +
      labs(
        x = "Number of tourists \n(thousands)",
        y = "Residuals"
        )

    p_3 <- ggplot(m_aug, aes(x = .resid)) +
      geom_histogram(binwidth = 250) +
      labs(
        x = "Residuals",
        y = NULL
        )

    p_1 + p_2 + p_3
    ```

    a.  Describe the relationship between number of tourists and spending.

    b.  What are the predictor and the outcome variables?

    c.  Why might we want to fit a regression line to these data?

    d.  Do the data meet the LINE conditions required for fitting a least squares line?
        Use the scatterplot, the residuals plot, and the histogram to answer this question.

\vfill

\clearpage

3.  **Cherry trees, collinear predictors.** Timber yield is approximately equal to the volume of a tree, however, this value is difficult to measure without first cutting the tree down.
    Instead, other variables, such as height and diameter, may be used to predict a tree's volume and yield.
    Researchers wanting to understand the relationship between these variables for black cherry trees collected data from 31 such trees in the Allegheny National Forest, Pennsylvania.
    Height is measured in feet, diameter in inches (at 54 inches above ground), and volume in cubic feet.[^_25-ex-inf-model-mlr-2]
    [@Hand:1994]
    The plots below display the distribution of each of these variables (on the diagonal) as well as provide information on the pairwise correlations between them.

    ```{r}
    cherry |>
      ggpairs(diag = list(continuous = "barDiag"),
              switch = "both") +
      theme(legend.position = "none",
        panel.grid.major = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_text(angle = 180, vjust = 1, color = "black"),
        panel.border = element_rect(fill = NA))
    ```

    Also provided below are three regression model outputs: `volume` vs. `diam`, `volume` vs. `height`, and `volume` vs. `height + diam`.
    
    \vspace{-2mm}

    ```{r}
    lm(volume ~ diam, data = cherry) |> 
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 3) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```
    
    \vspace{-7mm}
    
    ```{r}
    lm(volume ~ height, data = cherry) |> 
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 3) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```
    
    \vspace{-7mm}
    
    ```{r}
    lm(volume ~ height + diam, data = cherry) |> 
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 3) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```

    \vspace{-5mm}

    a.  There are three variables described in the figure, and each is paired with each other to create three different scatterplots.
        Rate the pairwise relationships from most correlated to least correlated.

    b.  When using only one variable to model a tree's `volume`, is `diam`eter a discernible predictor?
        Is `height` a discernible predictor?
        Explain your reasoning.

    c.  When using both `diam`eter and `height` to predict a tree's `volume`, are both predictors still discernible?
        Explain your reasoning.

\clearpage

4.  **GPA, collinear predictors.** In this exercise we work with data from a survey of 55 Duke University students who were asked about their GPA, number of hours they sleep nightly, and number of nights they go `out` each week.
    The plots below display the distributions of each of these variables (on the diagonal) as well as their pairwise relationships and correlation coefficients.

    ```{r}
    #| out-width: 90%
    gpa |>
      select(sleepnight, out, gpa) |>
      ggpairs(diag = list(continuous = "barDiag"),
              switch = "both") +
      theme(legend.position = "none",
        panel.grid.major = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_text(angle = 180, vjust = 1, color = "black"),
        panel.border = element_rect(fill = NA))
    ```

    Also provided below are three regression model outputs: `gpa` vs. `out`, `gpa` vs. `sleepnight`, and `gpa` vs. `out + sleepnight`.

    ```{r}
    lm(gpa ~ out, data = gpa) |>
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 3) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```

    \vspace{-5mm}
    
    ```{r}
    lm(gpa ~ sleepnight, data = gpa) |>
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 3) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```
    
    \vspace{-5mm}
    
    ```{r}
    lm(gpa ~ out + sleepnight, data = gpa) |>
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 3) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```

    a.  There are three variables described in the figure, and each is paired with each other to create three different scatterplots.
        Rate the pairwise relationships from most correlated to least correlated.

    b.  When using only one variable to model `gpa`, is `out` a discernible predictor?
        Is `sleepnight` a discernible predictor?
        Explain your reasoning.

    c.  When using both `out` and `sleepnight` to predict `gpa` in a multiple regression model, are either of the variables discernible?
        Explain your reasoning.

\clearpage

5.  **Movie returns.** A FiveThirtyEight.com article reports that "Horror movies get nowhere near as much draw at the box office as the big-time summer blockbusters or action/adventure movies, but there's a huge incentive for studios to continue pushing them out. The return-on-investment potential for horror movies is absurd." To investigate how the return-on-investment (ROI) compares between genres and how this relationship has changed over time, an introductory statistics student fit a linear regression model to predict the ratio of gross revenue of movies to the production costs from genre and release year for 1,070 movies released between 2000 and 2018.
    Using the plots given below, determine if this regression model is appropriate for these data.
    In particular, use the residual plot to check the LINE conditons.
    [@webpage:horrormovies]

    ```{r}
    #| out-width: 100%
    movie_profit <- read_csv("data/movie_profit.csv")
    movie_profit <- movie_profit |>
      mutate(
        release_date = mdy(release_date),
        release_year = year(release_date),
        oct_release = ifelse(month(release_date) == 10, "yes", "no"),
        dom_gross_to_prod = domestic_gross / production_budget,
        ww_gross_to_prod = worldwide_gross / production_budget
        ) |>
      filter(
        release_year >= 2010,
        release_year < 2019
      )

    m_movie <- lm(ww_gross_to_prod ~ release_year + genre, data = movie_profit)
    m_movie_aug <- augment(m_movie) |>
      rownames_to_column()

    p_res_hist <- ggplot(m_movie_aug, aes(x = .resid)) +
      geom_histogram() +
      labs(x = "Residual", y = "Count", title = "Histogram of residuals")

    p_res_order <- ggplot(m_movie_aug, aes(y = .resid, x = as.numeric(rowname))) +
      geom_point() +
      labs(x = "Data order", y = "Residual", title = "Residuals vs. data order")

    p_res_pred <- ggplot(m_movie_aug, aes(x = .fitted, y = .resid, color = genre)) +
      geom_point(alpha = 0.8, size = 1) +
      labs(x = "Predicted ROI", y = "Residual", color = "Genre", title = "Residuals vs. predicted values") +
      scale_color_openintro("five") +
      theme(
        legend.position = c(0.4, 0.8),
        legend.direction = "horizontal",
        legend.background = element_rect(color = "gray", fill = "white")
      )

    (p_res_hist + p_res_order) /
        p_res_pred
    ```

\vfill

6.  **Difficult encounters.** A study was conducted at a university outpatient primary care clinic in Switzerland to identify factors associated with difficult doctor-patient encounters.
    The data consist of 527 patient encounters, conducted by the 27 medical residents employed at the clinic.
    After each encounter, the attending physician completed two questionnaires: the Difficult Doctor Patient Relationship Questionnaire (DDPRQ-10) and the Patient's Vulnerability Grid (PVG).
    A higher score on the DDPRQ-10 indicates a more difficult encounter.
    The maximum possible score is 60 and encounters with score 30 and higher are considered difficult.
    A model was fit to predict DDPRQ-10 score from features of the attending physician: age, sex (male or not), and years of training.

    ```{r}
    tribble(
      ~term,         ~estimate, ~std.error, ~statistic, ~p.value,
      "(Intercept)",    30.594,      2.886,     10.601,   0.0000,
      "age",            -0.016,      0.104,     -0.157,    0.876,
      "sexMale",        -0.535,      0.781,     -0.686,    0.494,
      "yrs_train",       0.096,      0.215,      0.445,    0.656
    ) |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 3) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```

    a.  The intercept of the model is 30.594.
        What is the age, sex, and years of training of a physician whom this model would predict to have a DDPRQ-10 score of 30.594.

    b.  Is there evidence of a discernible association between DDPRQ-10 score and any of the physician features?

\vfill

\clearpage

7.  **Baby's weight, mathematical test.** US Department of Health and Human Services, Centers for Disease Control and Prevention collect information on births recorded in the country.
    The data used here are a random sample of 1,000 births from 2014.
    Here, we study the relationship between smoking and weight of the baby.
    The variable `smoke` is coded 1 if the mother is a smoker, and 0 if not.
    The summary table below shows the results of a linear regression model for predicting the average birth weight of babies, measured in pounds, based on the smoking status of the mother.[^_25-ex-inf-model-mlr-4]
    [@data:births14]

    ```{r}
    m_babies_full <- lm(weight ~ weeks + mage + sex + visits + habit, data = births14)

    m_babies_full |>
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 2) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```

    Also shown below are a series of diagnostics plots.

    ```{r}
    #| fig-asp: 0.8
    m_babies_full_aug <- augment(m_babies_full)

    p_res_hist <- ggplot(m_babies_full_aug, aes(x = .resid)) +
      geom_histogram(binwidth = 0.5) +
      labs(x = "Residual", y = "Count", title = "Histogram of residuals")

    p_res_order <- ggplot(m_babies_full_aug, aes(y = .resid, x = as.numeric(.rownames))) +
      geom_point() +
      labs(x = "Data order", y = "Residual", title = "Residuals vs. data order")

    p_res_pred_1 <- ggplot(m_babies_full_aug, aes(x = .fitted, y = .resid)) +
      geom_point(alpha = 0.8, size = 1) +
      labs(x = "Predicted weight", y = "Residual", title = "Residuals vs. predicted values")

    p_res_pred_2 <- ggplot(m_babies_full_aug |> filter(.fitted > 6 & .fitted < 8.5), aes(x = .fitted, y = .resid)) +
      geom_point(alpha = 0.8, size = 1) +
      labs(x = "Predicted weight", y = "Residual", 
           title = "Residuals vs. predicted values",
           subtitle = "Predicted weights between 6 and 8.5 pounds")

    (p_res_hist + p_res_order) /
      (p_res_pred_1 + p_res_pred_2)
    ```

    a.  Determine if the conditions for doing inference based on mathematical models with these data are met using the diagnostic plots above.
        If not, describe how to proceed with the analysis.

    b.  Using the regression output, evaluate whether the true slope of `habit` (i.e., whether the mother is a smoker) is different than 0, given the other variables in the model.
        State the null and alternative hypotheses, report the p-value (using a mathematical model), and state your conclusion.

\clearpage

8.  **Baby's weight, collinear predictors.** In this exercise we study the relationship between the weight of the baby and two explanatory variables: number of `weeks` of gestation and number of pregnancy hospital `visits`.
    [@data:births14]
    The plots below display the distributions of each of these variables (on the diagonal) as well as their pairwise relationships and correlation coefficients.

    ```{r}
    #| out-width: 90%
    set.seed(470)
    births14_100 <- births14 |>
      select(weeks, visits, weight) |>
      drop_na() |>
      sample_n(100) 
    births14_100 |>
      ggpairs(diag = list(continuous = "barDiag"),
              switch = "both") +
      theme(legend.position = "none",
        panel.grid.major = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_text(angle = 180, vjust = 1, color = "black"),
        panel.border = element_rect(fill = NA))
    ```

    Also provided below are three regression model outputs: `weight` vs. `weeks`, `weight` vs. `visits`, and `weight` vs. `weeks + visits`.

    ```{r}
    lm(weight ~ weeks, data = births14_100) |> 
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 2) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```

    \vspace{-5mm}
    
    ```{r}
    lm(weight ~ visits, data = births14_100) |> 
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 2) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```
    
    \vspace{-5mm}
    
    ```{r}
    lm(weight ~ weeks + visits, data = births14_100) |> 
      tidy() |>
      mutate(p.value = ifelse(p.value < 0.0001, "<0.0001", round(p.value, 4))) |>
      kbl(linesep = "", booktabs = TRUE, align = "lrrrr", digits = 2) |>
      kable_styling(bootstrap_options = c("striped", "condensed"), 
                    latex_options = "HOLD_position",
                    full_width = FALSE) |>
      column_spec(1, width = "10em", monospace = TRUE) |>
      column_spec(2:5, width = "5em")
    ```

    a.  There are three variables described in the figure, and each is paired with each other to create three different scatterplots.
        Rate the pairwise relationships from most correlated to least correlated.

    b.  When using only one variable to model the baby's `weight`, is `weeks` a discernible predictor?
        Is `visits` a discernible predictor?
        Explain your reasoning.

    c.  When using both `visits` and `weeks` to predict the baby's `weight`, are both predictors still discernible?
        Explain your reasoning.

\clearpage

9.  **Baby's weight, cross-validation.** Using a random sample of 1,000 US births from 2014, we study the relationship between the weight of the baby and various explanatory variables.
    [@data:births14]
    The plots below display prediction errors associated with two different models designed to predict `weight` of baby at birth; one model uses 7 predictors, one model uses 2 predictors.
    Using 4-fold cross-validation, the data were split into 4 folds.
    Three of the folds estimate the $\beta_i$ parameters using $b_i$, and the model is applied to the held out fold for prediction.
    The process was repeated 4 times (each time holding out one of the folds).

    $$
    \begin{aligned}
    E[\texttt{weight}] = \beta_0 &+ \beta_1\times \texttt{fage} + \beta_2\times \texttt{mage} + \beta_3 \times \texttt{mature} + \beta_4 \times \texttt{weeks} \\
    &+ \beta_5 \times \texttt{visits} + \beta_6 \times \texttt{gained} + \beta_7 \times \texttt{habit}\\
    E[\texttt{weight}] = \beta_0 &+ \beta_1\times \texttt{weeks} + \beta_2\times \texttt{mature}
    \end{aligned}
    $$

    ```{r}
    #| fig-asp: 0.9
    set.seed(470)
    births14_100 <- births14 |>
      drop_na() |>
      sample_n(100)

    set.seed(4)
    births_CV1 <- train(weight ~ fage + mage + mature + weeks +
      visits + gained + habit,
    data = births14_100,
    na.action = na.omit,
    method = "lm",
    trControl = trainControl(
      method = "cv", number = 4,
      savePredictions = TRUE
    )
    )
    births_CV2 <- train(weight ~ weeks + mature,
      data = births14_100,
      na.action = na.omit,
      method = "lm",
      trControl = trainControl(
        method = "cv", number = 4,
        savePredictions = TRUE
      )
    )

    SSE_CV1 <- births_CV1$pred |>
      mutate(resid = obs - pred) |>
      select(resid) |>
      pull() %>%
      `^`(2) |>
      sum() |>
      round(2)

    SSE_CV2 <- births_CV2$pred |>
      mutate(resid = obs - pred) |>
      select(resid) |>
      pull() %>%
      `^`(2) |>
      sum() |>
      round(2)

    p1 <- births_CV1$pred |>
      mutate(resid = obs - pred) |>
      ggplot() +
      geom_point(aes(x = pred, y = resid, color = Resample, shape = Resample), alpha = 0.8, size = 2) +
      geom_rug(aes(y = resid), color = IMSCOL["gray", "full"]) +
      geom_hline(yintercept = 0) +
      ylim(c(-5, 5)) +
      xlim(c(2, 11.5)) +
      labs(
        title = "Model with 7 predictors",
        color = "CV Fold", shape = "CV Fold",
        x = "Predicted", y = "Prediction error\n(residual)"
      ) +
      annotate(geom = "text", x = 9, y = 4, hjust = 0, label = paste("CV SSE =", SSE_CV1)) +
      scale_color_openintro("four") +
      scale_shape_manual(values = c(15, 8, 17, 19))

    p2 <- births_CV2$pred |>
      mutate(resid = obs - pred) |>
      ggplot() +
      geom_point(aes(x = pred, y = resid, color = Resample, shape = Resample), alpha = 0.8, size = 2) +
      geom_rug(aes(y = resid), color = IMSCOL["gray", "full"]) +
      geom_hline(yintercept = 0) +
      ylim(c(-5, 5)) +
      xlim(c(2, 11.5)) +
      labs(
        title = "Model with 2 predictors",
        color = "CV Fold", shape = "CV Fold",
        x = "Predicted", y = "Prediction error\n(residual)"
      ) +
      annotate(geom = "text", x = 9, y = 4, hjust = 0, label = paste("CV SSE =", SSE_CV2)) +
      scale_color_openintro("four") +
      scale_shape_manual(values = c(15, 8, 17, 19))

    p1 / p2 + plot_layout(guides = "collect") & 
      theme(legend.position = "bottom")
    ```

    a.  In the first graph, note the point at roughly (predicted = 11 and error = -4).
        Estimate the observed and predcited value for that observation.

    b.  Using the same point, describe which cross-validation fold(s) were used to build its prediction model.

    c.  For the plot on the top, for one of the cross-validation folds, how many coefficients were estimated in the linear model?
        For the plot on the bottom, for one of the cross-validation folds, how many coefficients were estimated in the linear model?

    d.  Do the values of the residuals (along the y-axis, not the x-axis) seem markedly different for the two models?
        Explain your reasoning.

\clearpage

10. **RailTrail, cross-validation.** The Pioneer Valley Planning Commission (PVPC) collected data north of Chestnut Street in Florence, MA for ninety days from April 5, 2005 to November 15, 2005.
    Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.[^_25-ex-inf-model-mlr-5]
    The plots below display prediction errors associated with two different models designed to predict the `volume` of riders on the RailTrail; one model uses 6 predictors, one model uses 2 predictors.
    Using 3-fold cross-validation, the data were split into 3 folds.
    Three of the folds estimate the $\beta_i$ parameters using $b_i$, and the model is applied to the held out fold for prediction.
    The process was repeated 4 times (each time holding out one of the folds).

    $$
    \begin{aligned}
    E[\texttt{volume}] = \beta_0 &+ \beta_1 \times \texttt{hightemp} + \beta_2 \times \texttt{lowtemp} + \beta_3 \times \texttt{spring}\\
    &+ \beta_4 \times \texttt{weekday} + \beta_5 \times \texttt{cloudcover}+ \beta_6 \times \texttt{precip} \\
    E[\texttt{volume}] = \beta_0 &+ \beta_1\times \texttt{hightemp} + \beta_2\times \texttt{precip}
    \end{aligned}
    $$

    ```{r}
    #| fig-asp: 0.9
    set.seed(4)
    rail_CV1 <- train(volume ~ hightemp + lowtemp + spring + cloudcover + precip + weekday,
      data = RailTrail,
      na.action = na.omit,
      method = "lm",
      trControl = trainControl(
        method = "cv", number = 3,
        savePredictions = TRUE
      )
    )
    rail_CV2 <- train(volume ~ hightemp + precip,
      data = RailTrail,
      na.action = na.omit,
      method = "lm",
      trControl = trainControl(
        method = "cv", number = 3,
        savePredictions = TRUE
      )
    )

    SSE_CV1 <- rail_CV1$pred |>
      mutate(resid = obs - pred) |>
      select(resid) |>
      pull() %>%
      `^`(2) |>
      sum() |>
      format(0, big.mark = ",")

    SSE_CV2 <- rail_CV2$pred |>
      mutate(resid = obs - pred) |>
      select(resid) |>
      pull() %>%
      `^`(2) |>
      sum() |>
      format(0, big.mark = ",")

    p1 <- rail_CV1$pred |>
      mutate(resid = obs - pred) |>
      ggplot() +
      geom_point(aes(x = pred, y = resid, color = Resample, shape = Resample), alpha = 0.8, size = 2) +
      geom_rug(aes(y = resid), color = IMSCOL["gray", "full"]) +
      geom_hline(yintercept = 0) +
      ylim(c(-300, 350)) +
      xlim(c(0, 620)) +
      labs(
        title = "Model with 6 predictors",
        color = "CV Fold", shape = "CV Fold",
        x = "Predicted", y = "Prediction error\n(residual)"
      ) +
      annotate(geom = "text", x = 20, y = -200, hjust = 0, label = paste("CV SSE =", SSE_CV1)) +
      scale_color_openintro("three")

    p2 <- rail_CV2$pred |>
      mutate(resid = obs - pred) |>
      ggplot() +
      geom_point(aes(x = pred, y = resid, color = Resample, shape = Resample), alpha = 0.8, size = 2) +
      geom_rug(aes(y = resid), color = IMSCOL["gray", "full"]) +
      geom_hline(yintercept = 0) +
      ylim(c(-300, 350)) +
      xlim(c(0, 620)) +
      labs(
        title = "Model with 2 predictors",
        color = "CV Fold", shape = "CV Fold",
        x = "Predicted", y = "Prediction error\n(residual)"
      ) +
      annotate(geom = "text", x = 20, y = -200, hjust = 0, label = paste("CV SSE =", SSE_CV2)) +
      scale_color_openintro("three")

    p1 / p2 + plot_layout(guides = "collect") &
      theme(legend.position = "bottom")
    ```

    a.  In the second graph, note the point at roughly (predicted = 400 and error = 100).
        Estimate the observed and predcited value for that observation.

    b.  Using the same point, describe which cross-validation fold(s) were used to build its prediction model.

    c.  For the plot on the top, for one of the cross-validation folds, how many coefficients were estimated in the linear model?
        For the plot on the bottom, for one of the cross-validation folds, how many coefficients were estimated in the linear model?

    d.  Do the values of the residuals (along the y-axis, not the x-axis) seem markedly different for the two models?
        Explain your reasoning.

\clearpage

11. **Baby's weight, cross-validation for model selection.** Using a random sample of 1,000 US births from 2014, we study the relationship between the weight of the baby and various explanatory variables.
    [@data:births14]
    The plots below display prediction errors associated with two different models designed to predict `weight` of baby at birth; one model uses 7 predictors, one model uses 2 predictors.
    Using 4-fold cross-validation, the data were split into 4 folds.
    Three of the folds estimate the $\beta_i$ parameters using $b_i$, and the model is applied to the held out fold for prediction.
    The process was repeated 4 times, each time holding out one of the folds.

    $$
    \begin{aligned}
    E[\texttt{weight}] = \beta_0 &+ \beta_1\times \texttt{fage} + \beta_2\times \texttt{mage}\\
    &+ \beta_3 \times \texttt{mature} + \beta_4 \times \texttt{weeks}\\\
    &+ \beta_5 \times \texttt{visits}+ \beta_6 \times \texttt{gained}\\
    &+ \beta_7 \times \texttt{habit}\\
    \end{aligned}
    $$

    $$
    \begin{aligned}
    E[\texttt{weight}] = \beta_0 &+ \beta_1\times \texttt{weeks} + \beta_2\times \texttt{mature}
    \end{aligned}
    $$

    ```{r}
    #| fig-asp: 1
    set.seed(470)
    births14_100 <- births14 |>
      drop_na() |>
      sample_n(100) 

    set.seed(4)
    births_CV1 <- train(weight ~ fage + mage + mature + weeks +
                           visits + gained  + habit,
                       data = births14_100,
                       na.action = na.omit,
                       method = "lm",
                       trControl = trainControl(method = "cv", number = 4,
                                                savePredictions = TRUE))
    births_CV2 <- train(weight ~ weeks + mature,
                       data = births14_100,
                       na.action = na.omit,
                       method = "lm",
                       trControl = trainControl(method = "cv", number = 4,
                                                savePredictions = TRUE))

    SSE_CV1 <- births_CV1$pred |> 
      mutate(resid = obs - pred) |>
      select(resid) |> pull() %>% `^`(2) |> sum() |> round(2)

    SSE_CV2 <- births_CV2$pred |> 
      mutate(resid = obs - pred) |>
      select(resid) |> pull() %>% `^`(2) |> sum() |> round(2)

    p1 <- births_CV1$pred |> 
      mutate(resid = obs - pred) |>
      ggplot() + 
      geom_point(aes(x = pred, y = resid, color = Resample, shape = Resample), alpha = 0.8, size = 2) +
      geom_rug(aes(y = resid), color = IMSCOL["gray", "full"]) +
      geom_hline(yintercept = 0) +
      ylim(c(-5,5)) +
      xlim(c(2,11.5)) +
      labs(
        title = "Model with 7 predictors",
        color = "CV Fold", shape = "CV Fold",
        x = "Predicted", y = "Prediction error\n(residual)"
        ) +
      annotate(geom = "text", x = 9, y = 4, hjust = 0, label = paste("CV SSE =", SSE_CV1)) +
      scale_color_openintro("four") +
      scale_shape_manual(values = c(15, 8, 17, 19))

    p2 <- births_CV2$pred |> 
      mutate(resid = obs - pred) |>
      ggplot() + 
      geom_point(aes(x = pred, y = resid, color = Resample, shape = Resample), alpha = 0.8, size = 2) +
      geom_rug(aes(y = resid), color = IMSCOL["gray", "full"]) +
      geom_hline(yintercept = 0) +
      ylim(c(-5,5)) +
      xlim(c(2,11.5)) +
      labs(
        title = "Model with 2 predictors",
        color = "CV Fold", shape = "CV Fold", 
        x = "Predicted", y = "Prediction error\n(residual)"
        ) +
      annotate(geom = "text", x = 9, y = 4, hjust = 0, label = paste("CV SSE =", SSE_CV2)) +
      scale_color_openintro("four") +
      scale_shape_manual(values = c(15, 8, 17, 19))

    p1 / p2 + plot_layout(guides = "collect") & 
      theme(legend.position = "bottom")
    ```

    a.  Using the spread of the points (in the y-direction), which model should be chosen for a final report on these data?
        Explain your reasoning.

    b.  Using the summary statistic (CV SSE), which model should be chosen for a final report on these data?
        Explain your reasoning.

    c.  Why would the model with more predictors fit the data less closely than the model with only two predictors?

\clearpage

12. **RailTrail, cross-validation for model selection.** The Pioneer Valley Planning Commission (PVPC) collected data north of Chestnut Street in Florence, MA for ninety days from April 5, 2005 to November 15, 2005.
    Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.
    The plots below display prediction errors associated with two different models designed to predict the `volume` of riders on the RailTrail; one model uses 6 predictors, one model uses 2 predictors.
    Using 3-fold cross-validation, the data were split into 3 folds.
    Three of the folds estimate the $\beta_i$ parameters using $b_i$, and the model is applied to the held out fold for prediction.
    The process was repeated 4 times, each time holding out one of the folds.

    $$
    \begin{aligned}
    E[\texttt{volume}] = \beta_0 &+ \beta_1\times \texttt{hightemp} + \beta_2\times \texttt{lowtemp}\\
    &+ \beta_3 \times \texttt{spring} + \beta_4 \times \texttt{weekday}\\\
    &+ \beta_5 \times \texttt{cloudcover}+ \beta_6 \times \texttt{precip}\\
    \end{aligned}
    $$

    $$
    \begin{aligned}
    E[\texttt{volume}] = \beta_0 &+ \beta_1\times \texttt{hightemp} + \beta_2\times \texttt{precip}
    \end{aligned}
    $$

    ```{r}
    #| fig-asp: 1
    set.seed(4)
    rail_CV1 <- train(volume ~ hightemp + lowtemp + spring + cloudcover + precip + weekday,
      data = RailTrail,
      na.action = na.omit,
      method = "lm",
      trControl = trainControl(
        method = "cv", number = 3,
        savePredictions = TRUE
      )
    )
    rail_CV2 <- train(volume ~ hightemp + precip,
      data = RailTrail,
      na.action = na.omit,
      method = "lm",
      trControl = trainControl(
        method = "cv", number = 3,
        savePredictions = TRUE
      )
    )

    SSE_CV1 <- rail_CV1$pred |>
      mutate(resid = obs - pred) |>
      select(resid) |>
      pull() %>%
      `^`(2) |>
      sum() |>
      format(0, big.mark = ",")

    SSE_CV2 <- rail_CV2$pred |>
      mutate(resid = obs - pred) |>
      select(resid) |>
      pull() %>%
      `^`(2) |>
      sum() |>
      format(0, big.mark = ",")

    p1 <- rail_CV1$pred |>
      mutate(resid = obs - pred) |>
      ggplot() +
      geom_point(aes(x = pred, y = resid, color = Resample, shape = Resample), alpha = 0.8, size = 2) +
      geom_rug(aes(y = resid), color = IMSCOL["gray", "full"]) +
      geom_hline(yintercept = 0) +
      ylim(c(-300, 350)) +
      xlim(c(0, 620)) +
      labs(
        title = "Model with 6 predictors",
        color = "CV Fold", shape = "CV Fold",
        x = "Predicted", y = "Prediction error\n(residual)"
      ) +
      annotate(geom = "text", x = 20, y = -200, hjust = 0, label = paste("CV SSE =", SSE_CV1)) +
      scale_color_openintro("three")

    p2 <- rail_CV2$pred |>
      mutate(resid = obs - pred) |>
      ggplot() +
      geom_point(aes(x = pred, y = resid, color = Resample, shape = Resample), alpha = 0.8, size = 2) +
      geom_rug(aes(y = resid), color = IMSCOL["gray", "full"]) +
      geom_hline(yintercept = 0) +
      ylim(c(-300, 350)) +
      xlim(c(0, 620)) +
      labs(
        title = "Model with 2 predictors",
        color = "CV Fold", shape = "CV Fold",
        x = "Predicted", y = "Prediction error\n(residual)"
      ) +
      annotate(geom = "text", x = 20, y = -200, hjust = 0, label = paste("CV SSE =", SSE_CV2)) +
      scale_color_openintro("three")


    p1 / p2 + plot_layout(guides = "collect") &
      theme(legend.position = "bottom")
    ```

    a.  Using the spread of the points (in the y-direction), which model should be chosen for a final report on these data?
        Explain your reasoning.

    b.  Using the summary statistic (CV SSE), which model should be chosen for a final report on these data?
        Explain your reasoning.

    c.  Why would the model with more predictors fit the data less closely than the model with only two predictors?

[^_25-ex-inf-model-mlr-1]: The [`gpa`](http://openintrostat.github.io/openintro/reference/gpa.html) data used in this exercise can be found in the [**openintro**](http://openintrostat.github.io/openintro) R package.

[^_25-ex-inf-model-mlr-2]: The [`cherry`](http://openintrostat.github.io/openintro/reference/cherry.html) data used in this exercise can be found in the [**openintro**](http://openintrostat.github.io/openintro) R package.

[^_25-ex-inf-model-mlr-3]: The [`tourism`](http://openintrostat.github.io/openintro/reference/tourism.html) data used in this exercise can be found in the [**openintro**](http://openintrostat.github.io/openintro) R package.

[^_25-ex-inf-model-mlr-4]: The [`births14`](http://openintrostat.github.io/openintro/reference/births14.html) data used in this exercise can be found in the [**openintro**](http://openintrostat.github.io/openintro) R package.

[^_25-ex-inf-model-mlr-5]: The [`RailTrail`](https://rdrr.io/cran/mosaicData/man/RailTrail.html) data used in this exercise can be found in the [**mosaicData**](https://cran.r-project.org/web/packages/mosaicData/index.html) R package.
